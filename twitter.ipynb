{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import pymongo\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from loguru import logger\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException,StaleElementReferenceException,TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "\n",
    "    def __init__(self, query, uid, ptime, pcontent, padditional, nb_reply, nb_retweet, nb_favorite):\n",
    "        self.query = query\n",
    "        self.uid = uid\n",
    "        self.ptime = ptime\n",
    "        self.pcontent = pcontent\n",
    "        self.padditional = padditional  # 转发推文，文章链接，图片，视频\n",
    "        self.nb_retweet = nb_retweet  # nbr of retweet\n",
    "        self.nb_favorite = nb_favorite  # nbr of favorite\n",
    "        self.nb_reply = nb_reply    # nbr of reply\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Tweet={}\\nQuery={}\".format(self.pcontent, self.query)\n",
    "\n",
    "\n",
    "class User:\n",
    "\n",
    "    def __init__(self, profile_url):\n",
    "        self.profile_url = profile_url\n",
    "        self.ID = profile_url.split('/')[-1]\n",
    "        self.name = ''\n",
    "        self.avatar = ''\n",
    "        self.query = ''# query相关的大V用户\n",
    "        self.intro = ''\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"User {}\".format(self.ID)\n",
    "\n",
    "\n",
    "def compare_time(time1,time2):\n",
    "    s_time = time.mktime(time.strptime(time1,'%Y年%m月%d日'))\n",
    "    e_time = time.mktime(time.strptime(time2,'%Y年%m月%d日'))\n",
    "    return int(s_time) - int(e_time)\n",
    "    \n",
    "def convert_time(x):\n",
    "    '''\n",
    "    for x in ['20分钟','1小时','1天', '10月10日','2018年10月1日']:\n",
    "        print(convert_time(x))\n",
    "    '''\n",
    "    now = datetime.datetime.now()\n",
    "    pattern = r'\\d{4}年\\d+月\\d+日'\n",
    "    if re.match(pattern, x):\n",
    "        return x\n",
    "    pattern = r'\\d+月\\d+日'\n",
    "    if re.match(pattern, x):\n",
    "        return \"{}年\".format(now.year)+x\n",
    "    return \"{}年{}月{}日\".format(now.year, now.month, now.day)\n",
    "\n",
    "def is_non_result(browser):\n",
    "    '''\n",
    "    判断结果是否为空\n",
    "    '''\n",
    "#     result_div_xpath = \"//div[@id='react-root']\"\n",
    "#     wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "#     try:\n",
    "#         result_div = browser.find_element_by_xpath(result_div_xpath)\n",
    "#         return \"未找到结果\" in result_div.text\n",
    "#     except NoSuchElementException as e:\n",
    "    return \"未找到结果\" in browser.find_element_by_tag_name('body').text\n",
    "\n",
    "def get_search_input_v1(browser):\n",
    "    # 定位搜索框\n",
    "    search_input_xpath = \"//input[@placeholder='搜索 Twitter']\"\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, search_input_xpath)))\n",
    "    search_input = browser.find_element_by_xpath(search_input_xpath)\n",
    "    return search_input\n",
    "\n",
    "# def get_search_input_v2(browser):\n",
    "#     # 请求主站\n",
    "#     browser.get('https://twitter.com/search-home')\n",
    "#     # 定位搜索框\n",
    "#     search_input_id = 'search-home-input'\n",
    "#     wait.until(EC.presence_of_element_located((By.ID, search_input_id)))\n",
    "#     search_input = browser.find_element_by_id(search_input_id)\n",
    "#     return search_input\n",
    "\n",
    "\n",
    "def extract_reply_retweet_favorite(element):\n",
    "    t = []\n",
    "    for x in element.find_elements_by_xpath('./div')[:3]:\n",
    "        if x.text.strip() == '':\n",
    "            t.append(0)\n",
    "        else:\n",
    "            t.append(int(x.text.strip()))\n",
    "    return tuple(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query -> 推文爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweet_result_div(result_div,query):\n",
    "    count = 0\n",
    "    for div in result_div:\n",
    "        user, tweet = div.find_elements_by_xpath('./div')\n",
    "        profile_url = user.find_element_by_tag_name(\n",
    "            'a').get_attribute('href').strip()\n",
    "        uid = profile_url.split('/')[-1]\n",
    "        a, *b_c, d = tweet.find_elements_by_xpath('./div')  # 按照div分为>=3层\n",
    "        ptime = a.find_elements_by_tag_name('a')[-1].text\n",
    "        ptime = convert_time(ptime)\n",
    "        nb_reply, nb_retweet, nb_favorite = 0,0,0\n",
    "        try:\n",
    "            nb_reply, nb_retweet, nb_favorite = extract_reply_retweet_favorite(\n",
    "                d)\n",
    "        except:\n",
    "            nb_reply, nb_retweet, nb_favorite = 0, 0, 0\n",
    "        pcontent = b_c[0].text\n",
    "        padditional = []\n",
    "        if len(b_c) > 1:\n",
    "            for x in b_c[1:]:\n",
    "                try:\n",
    "                    a = x.find_element_by_tag_name('a').get_attribute('href')\n",
    "                    padditional.append(a)\n",
    "                except NoSuchElementException as e:\n",
    "                    padditional.append(x.text.strip())\n",
    "        user = User(profile_url)\n",
    "        tweet = Tweet(query, uid, ptime, pcontent, padditional,\n",
    "                      nb_reply, nb_retweet, nb_favorite)\n",
    "        # save to databse\n",
    "        user_dict = user.__dict__\n",
    "        user_dict['_id']=user_dict['ID']\n",
    "        if user_table.update_one({'_id': user_dict['_id']}, {'$set': user_dict}, upsert=True) and tweet_table.insert_one(tweet.__dict__):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def crawl_tweet(browser, query):\n",
    "    count = 0\n",
    "    result_div_xpath = '//div[@data-testid=\"tweet\"]'\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "    result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "    last_div = result_div[-1]\n",
    "    # 解析结果\n",
    "    count += parse_tweet_result_div(result_div,query)\n",
    "    while count < MAX_TWEET_SIZE:\n",
    "#         logger.info(\"{}/{}\".format(count,MAX_TWEET_SIZE))\n",
    "        result_div_xpath = '//div[@data-testid=\"tweet\"]'\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "        result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "        last_div = result_div[-1]\n",
    "        try:\n",
    "            count += parse_tweet_result_div(result_div,query)\n",
    "        except StaleElementReferenceException as e:\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        \n",
    "        # 翻页\n",
    "        try_times = 0\n",
    "        old_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "        while True:\n",
    "            browser.execute_script(\n",
    "                'window.scrollTo(0,document.body.scrollHeight)')\n",
    "            wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, result_div_xpath)))\n",
    "            result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "            if result_div[-1] != last_div:\n",
    "                last_div = result_div[-1]\n",
    "                break\n",
    "            time.sleep(3)\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "            if old_height == new_height:\n",
    "                try_times += 1\n",
    "            if try_times >= 3:\n",
    "                count = MAX_TWEET_SIZE # 到头了停止翻页采集该query\n",
    "                print('到头了')\n",
    "                break\n",
    "\n",
    "def search_tweet_from_query(browser,query_list,finish_query_list):\n",
    "    '''\n",
    "    更加query采集推文\n",
    "    '''\n",
    "    for query in tqdm(query_list):\n",
    "        logger.info('query = {}'.format(query))\n",
    "        browser.get('https://twitter.com/explore')\n",
    "\n",
    "        # 定位搜索框\n",
    "        if browser.current_url == 'https://twitter.com/explore':\n",
    "            search_input = get_search_input_v1(browser)\n",
    "        else:\n",
    "            print('error')\n",
    "            return\n",
    "        # 搜索query\n",
    "        search_input.clear()\n",
    "        search_input.send_keys(query)\n",
    "        search_input.send_keys(Keys.ENTER)\n",
    "\n",
    "        # 获取结果\n",
    "        if is_non_result(browser):\n",
    "            bad_query_list.append(query)\n",
    "            continue\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            crawl_tweet(browser, query)\n",
    "        except TimeoutException as e:\n",
    "            print('TimeoutException')\n",
    "            continue\n",
    "        finish_query_list.append(query)\n",
    "    print(bad_query_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query -> 爬取相关用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_user_from_query(browser,query_list,finish_query_list):\n",
    "    '''\n",
    "    根据query采集maxsize用户列表\n",
    "    '''\n",
    "    for query in tqdm(query_list):\n",
    "        logger.info('query = {}'.format(query))\n",
    "        browser.get('https://twitter.com/explore')\n",
    "\n",
    "        # 定位搜索框\n",
    "        if browser.current_url == 'https://twitter.com/explore':\n",
    "            search_input = get_search_input_v1(browser)\n",
    "        else:\n",
    "            print('error')\n",
    "            return\n",
    "        # 搜索query\n",
    "        search_input.clear()\n",
    "        search_input.send_keys(query)\n",
    "        search_input.send_keys(Keys.ENTER)\n",
    "\n",
    "        # 获取结果\n",
    "        if is_non_result(browser):\n",
    "            bad_query_list.append(query)\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "        # 请求用户结果页面\n",
    "        browser.get(browser.current_url + '&f=user')\n",
    "        \n",
    "        try:\n",
    "            crawl_user(browser,query)\n",
    "        except TimeoutException as e:\n",
    "            print('TimeoutException')\n",
    "            continue\n",
    "        finish_query_list.append(query)\n",
    "    \n",
    "    print(bad_query_list)\n",
    "\n",
    "def parse_user_result_div(result_div,query):\n",
    "    count = 0\n",
    "    for t in result_div:\n",
    "        left, right = t.find_elements_by_xpath('./div/div')\n",
    "        profile_url = left.find_element_by_tag_name(\n",
    "            'a').get_attribute('href').strip()\n",
    "        uid = profile_url.split('/')[-1]\n",
    "        print('parsing uid = {}'.format(uid))\n",
    "        avatar = left.find_element_by_tag_name('img').get_attribute('src')\n",
    "        a, *b = right.find_elements_by_xpath('./div')  # 按照div分为>=3层\n",
    "        uname= a.text.split('\\n')[0]\n",
    "        intro = ''\n",
    "        if len(b)!=0:\n",
    "            intro = b[-1].text.strip()\n",
    "        \n",
    "        user = User(profile_url)\n",
    "        user.ID = uid\n",
    "        user.avatar = avatar\n",
    "        user.name = uname\n",
    "        user.intro = intro\n",
    "        user.query = query\n",
    "        \n",
    "        user_dict = user.__dict__\n",
    "        user_dict['_id']=user_dict['ID']\n",
    "\n",
    "        # save to databse\n",
    "        if user_table.update_one({'_id': user_dict['_id']}, {'$set': user_dict}, upsert=True):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def crawl_user(browser, query):\n",
    "    count = 0\n",
    "    result_div_xpath = '//div[@data-testid=\"UserCell\"]'\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "    result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "    last_div = result_div[-1]\n",
    "    count += parse_user_result_div(result_div,query)\n",
    "    \n",
    "    while count < MAX_USER_SIZE:\n",
    "#         logger.info(\"{}/{}\".format(count,MAX_USER_SIZE))\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "        result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "        last_div = result_div[-1]\n",
    "        \n",
    "        try:\n",
    "            count += parse_user_result_div(result_div,query)\n",
    "            time.sleep(2)\n",
    "        except StaleElementReferenceException as e:\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        \n",
    "        # 翻页\n",
    "        try_times = 0\n",
    "        old_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "        while True:\n",
    "            browser.execute_script(\n",
    "                'window.scrollTo(0,document.body.scrollHeight)')\n",
    "            wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, result_div_xpath)))\n",
    "            result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "            if result_div[-1] != last_div:\n",
    "                last_div = result_div[-1]\n",
    "                break\n",
    "            time.sleep(3)\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "            if old_height == new_height:\n",
    "                try_times += 1\n",
    "            if try_times >= 3:\n",
    "                count = MAX_TWEET_SIZE # 到头了停止翻页采集该query\n",
    "                print('到头了')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 爬取特定用户的推文（时间间隔内）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweet_from_profile(browser,query_user_list,finish_user_list):\n",
    "    for user in tqdm(query_user_list):\n",
    "        logger.info('user = {}'.format(u['_id']))\n",
    "        user_profile = 'https://twitter.com/'+ u['_id']\n",
    "        browser.get(user_profile)\n",
    "\n",
    "        # 获取结果\n",
    "        if is_non_result(browser):\n",
    "            bad_query_list.append(query)\n",
    "            continue\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            crawl_tweet2(browser,query)\n",
    "        except TimeoutException as e:\n",
    "            print('TimeoutException')\n",
    "            continue\n",
    "        \n",
    "        finish_user_list.append(user)\n",
    "    print(bad_query_list)\n",
    "\n",
    "def crawl_tweet2(browser, query):\n",
    "    count = 0\n",
    "    result_div_xpath = '//div[@data-testid=\"tweet\"]'\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "    result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "    last_div = result_div[-1]\n",
    "    # 解析结果\n",
    "    count += parse_tweet_from_profile(result_div,query)\n",
    "    while count < MAX_TWEET_SIZE:\n",
    "#         logger.info(\"{}/{}\".format(count,MAX_TWEET_SIZE))\n",
    "        result_div_xpath = '//div[@data-testid=\"tweet\"]'\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "        result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "        last_div = result_div[-1]\n",
    "        try:\n",
    "            count += parse_tweet_from_profile(result_div,query)\n",
    "        except StaleElementReferenceException as e:\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        \n",
    "        # 翻页\n",
    "        try_times = 0\n",
    "        old_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "        while True:\n",
    "            browser.execute_script(\n",
    "                'window.scrollTo(0,document.body.scrollHeight)')\n",
    "            wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, result_div_xpath)))\n",
    "            result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "            if result_div[-1] != last_div:\n",
    "                last_div = result_div[-1]\n",
    "                break\n",
    "            time.sleep(3)\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "            if old_height == new_height:\n",
    "                try_times += 1\n",
    "            if try_times >= 3:\n",
    "                count = MAX_TWEET_SIZE # 到头了停止翻页采集该query\n",
    "                print('到头了')\n",
    "                break\n",
    "\n",
    "                \n",
    "def parse_tweet_from_profile(result_div,query):\n",
    "    count = 0\n",
    "    top = 0 # 置顶是个数目\n",
    "    # 如果存在置顶推文则不考虑时间\n",
    "    try:\n",
    "        t = browser.find_elements_by_xpath('//div[@class=\"css-1dbjc4n r-1habvwh r-1iusvr4 r-16y2uox r-5f2r5o\"]')\n",
    "        top = len(t)\n",
    "    except NoSuchElementException as e:\n",
    "        pass\n",
    "    \n",
    "    for div in result_div:\n",
    "        user, tweet = div.find_elements_by_xpath('./div')\n",
    "        profile_url = user.find_element_by_tag_name(\n",
    "            'a').get_attribute('href').strip()\n",
    "        uid = profile_url.split('/')[-1]\n",
    "        a, *b_c, d = tweet.find_elements_by_xpath('./div')  # 按照div分为>=3层\n",
    "        ptime = a.find_elements_by_tag_name('a')[-1].text\n",
    "        ptime = convert_time(ptime)\n",
    "        \n",
    "        # 无置顶推文则按照时间过滤\n",
    "        top -= 1\n",
    "        if top<0 and compare_time(ptime,time_interval) < 0:\n",
    "            print('触发时间截止')\n",
    "            return MAX_TWEET_SIZE # 使其直接达到数目规模，从而停止外层循环\n",
    "        \n",
    "        nb_reply, nb_retweet, nb_favorite = 0,0,0\n",
    "        try:\n",
    "            nb_reply, nb_retweet, nb_favorite = extract_reply_retweet_favorite(\n",
    "                d)\n",
    "        except:\n",
    "            nb_reply, nb_retweet, nb_favorite = 0, 0, 0\n",
    "        pcontent = b_c[0].text\n",
    "        padditional = []\n",
    "        if len(b_c) > 1:\n",
    "            for x in b_c[1:]:\n",
    "                try:\n",
    "                    a = x.find_element_by_tag_name('a').get_attribute('href')\n",
    "                    padditional.append(a)\n",
    "                except NoSuchElementException as e:\n",
    "                    padditional.append(x.text.strip())\n",
    "        tweet = Tweet(query, uid, ptime, pcontent, padditional,\n",
    "                      nb_reply, nb_retweet, nb_favorite)\n",
    "\n",
    "        if tweet_table.insert_one(tweet.__dict__):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 启动浏览器并登陆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://10.108.17.25:27017/\")\n",
    "twitter_db = client[\"twitter\"]\n",
    "user_table = twitter_db['user']\n",
    "tweet_table = twitter_db['tweet']\n",
    "\n",
    "# 打开浏览器\n",
    "browser = webdriver.Chrome()\n",
    "wait = WebDriverWait(browser, 100)\n",
    "\n",
    "# 人工登录\n",
    "browser.get('https://twitter.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.refresh()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 采集推文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./projects.csv',encoding='utf-8')\n",
    "df.columns = ['Project','Country','Type']\n",
    "query_list = [x.strip() for x in df['Project'].tolist() if len(x.split()) <= 5]\n",
    "query_list = query_list\n",
    "\n",
    "# df = pd.read_csv('./policies.csv',encoding='utf-8')\n",
    "# df.columns = ['P','_','__']\n",
    "# query_list = [x.strip() for x in df['P'].tolist() if len(x.split()) <= 10]\n",
    "# query_list = query_list\n",
    "len(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish_query_list = []\n",
    "# bad_query_list = []\n",
    "# MAX_TWEET_SIZE = 1000\n",
    "# search_tweet_from_query(browser,query_list,finish_query_list)\n",
    "\n",
    "MAX_TWEET_SIZE = 1000\n",
    "special_list = [\"the belt and road\",'One Belt one road',\"the Silk Road\",'the Silk Road Economic Belt']\n",
    "search_tweet_from_query(browser,special_list[1:],finish_query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finish_query_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 采集用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/108 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:27:27.368 | INFO     | __main__:search_user_from_query:6 - query = Padma Rail Link\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 1/108 [01:43<3:05:05, 103.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:29:11.156 | INFO     | __main__:search_user_from_query:6 - query = Lower Sesan Two Hydropower Dam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 2/108 [03:27<3:03:30, 103.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:30:55.225 | INFO     | __main__:search_user_from_query:6 - query = Central Asia–China gas pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/108 [05:12<3:02:05, 104.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:32:39.690 | INFO     | __main__:search_user_from_query:6 - query = Doraleh Multi-Purpose Port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = DoralehS\n",
      "parsing uid = DoralehS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 4/108 [05:29<2:15:12, 78.01s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:32:56.931 | INFO     | __main__:search_user_from_query:6 - query = Khorgos Gateway Dry Port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 5/108 [07:13<2:27:29, 85.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:34:41.306 | INFO     | __main__:search_user_from_query:6 - query = Forest City\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = knm0q0\n",
      "parsing uid = FCRSD\n",
      "parsing uid = FCIndianFB\n",
      "parsing uid = ForestCityTDC\n",
      "parsing uid = ForestCityIA\n",
      "parsing uid = ForestCityPD\n",
      "parsing uid = ForestCityFooty\n",
      "parsing uid = ForestCtComicon\n",
      "parsing uid = forestcityvelo\n",
      "parsing uid = ForestCityDerby\n",
      "parsing uid = LakeForestCA\n",
      "parsing uid = ForestCityOwls\n",
      "parsing uid = FCfilmfestival\n",
      "parsing uid = knm0q0\n",
      "parsing uid = FCRSD\n",
      "parsing uid = FCIndianFB\n",
      "parsing uid = ForestCityTDC\n",
      "parsing uid = ForestCityIA\n",
      "parsing uid = ForestCityPD\n",
      "parsing uid = ForestCityFooty\n",
      "parsing uid = ForestCtComicon\n",
      "parsing uid = forestcityvelo\n",
      "parsing uid = ForestCityDerby\n",
      "parsing uid = LakeForestCA\n",
      "parsing uid = ForestCityOwls\n",
      "parsing uid = FCfilmfestival\n",
      "parsing uid = ForestCityLovrs\n",
      "parsing uid = EppingForestCol\n",
      "parsing uid = ForestCityRaces\n",
      "parsing uid = OurCityForest\n",
      "parsing uid = FCNGolf\n",
      "parsing uid = ForestCityCook\n",
      "parsing uid = CLE_Jared\n",
      "parsing uid = FC_Summit\n",
      "parsing uid = ForestLake_MN\n",
      "parsing uid = ForestCityAFC\n",
      "parsing uid = FCSurplus\n",
      "parsing uid = fcssc\n",
      "parsing uid = fcbeerfest\n",
      "parsing uid = forestcityrcrds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 6/108 [07:30<1:50:33, 65.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:34:57.624 | INFO     | __main__:search_user_from_query:6 - query = Melaka Gateway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing uid = GatewayMelaka\n",
      "parsing uid = jelitasara\n",
      "parsing uid = MelakaGateway\n",
      "parsing uid = GatewayMelaka\n",
      "parsing uid = jelitasara\n",
      "parsing uid = MelakaGateway\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 7/108 [07:50<1:26:53, 51.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:35:17.918 | INFO     | __main__:search_user_from_query:6 - query = Pakistan-China – Fiber Optic Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 8/108 [09:35<1:52:51, 67.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:37:03.194 | INFO     | __main__:search_user_from_query:6 - query = Diamer-Bhasha Dam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 9/108 [11:19<2:09:45, 78.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:38:47.350 | INFO     | __main__:search_user_from_query:6 - query = Gwadar Port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = port_gwadar\n",
      "parsing uid = Gwadarpost\n",
      "parsing uid = GwadarPort1\n",
      "parsing uid = gwadar_port\n",
      "parsing uid = GwadarPortCity_\n",
      "parsing uid = gwadarport\n",
      "parsing uid = gwadar_portcity\n",
      "parsing uid = GwadarFreePort\n",
      "parsing uid = GwadarPortCity\n",
      "parsing uid = Gwadar_Sea_Port\n",
      "parsing uid = GWADAR_LOGISTIC\n",
      "parsing uid = gwadarcentral\n",
      "parsing uid = gwadarport13\n",
      "parsing uid = PortGwadar\n",
      "parsing uid = Gwadarport_\n",
      "parsing uid = port_gwadar\n",
      "parsing uid = Gwadarpost\n",
      "parsing uid = GwadarPort1\n",
      "parsing uid = gwadar_port\n",
      "parsing uid = GwadarPortCity_\n",
      "parsing uid = gwadarport\n",
      "parsing uid = gwadar_portcity\n",
      "parsing uid = GwadarFreePort\n",
      "parsing uid = GwadarPortCity\n",
      "parsing uid = Gwadar_Sea_Port\n",
      "parsing uid = GWADAR_LOGISTIC\n",
      "parsing uid = gwadarcentral\n",
      "parsing uid = gwadarport13\n",
      "parsing uid = PortGwadar\n",
      "parsing uid = Gwadarport_\n",
      "parsing uid = GovGwadar\n",
      "parsing uid = Gwadarport2030\n",
      "parsing uid = SaiyedShahzad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 10/108 [11:38<1:39:07, 60.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:39:06.151 | INFO     | __main__:search_user_from_query:6 - query = Belgrade-Montenegro Bar Port Motorway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n",
      "parsing uid = grad_beograd\n",
      "parsing uid = beogradEU\n",
      "parsing uid = grad_beograd\n",
      "parsing uid = beogradEU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 11/108 [11:55<1:16:53, 47.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:39:23.057 | INFO     | __main__:search_user_from_query:6 - query = Sino-Thai – High-Speed Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 12/108 [13:39<1:43:06, 64.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:41:06.917 | INFO     | __main__:search_user_from_query:6 - query = Colombo South Harbour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 13/108 [15:23<2:00:52, 76.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:42:51.022 | INFO     | __main__:search_user_from_query:6 - query = Port City Colombo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = PortCityColombo\n",
      "parsing uid = Kassapas\n",
      "parsing uid = ColomboPortCity\n",
      "parsing uid = PortCityColombo\n",
      "parsing uid = Kassapas\n",
      "parsing uid = ColomboPortCity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 14/108 [15:41<1:31:55, 58.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:43:08.452 | INFO     | __main__:search_user_from_query:6 - query = Hambantota Port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 15/108 [17:25<1:52:15, 72.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:44:52.987 | INFO     | __main__:search_user_from_query:6 - query = Single Gauge Trans-Asian Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 16/108 [19:09<2:05:32, 81.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:46:36.902 | INFO     | __main__:search_user_from_query:6 - query = Karuma Hydropower Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 17/108 [20:53<2:14:11, 88.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:48:20.778 | INFO     | __main__:search_user_from_query:6 - query = Pap Angren Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 18/108 [22:37<2:19:38, 93.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:50:04.647 | INFO     | __main__:search_user_from_query:6 - query = Budapest–Belgrade Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = grad_beograd\n",
      "parsing uid = beogradEU\n",
      "parsing uid = grad_beograd\n",
      "parsing uid = beogradEU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 19/108 [22:53<1:43:59, 70.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:50:21.110 | INFO     | __main__:search_user_from_query:6 - query = Yamal LNG Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▊        | 20/108 [24:38<1:58:05, 80.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:52:05.933 | INFO     | __main__:search_user_from_query:6 - query = Tehran-Mashhad Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 21/108 [26:22<2:07:05, 87.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:53:50.212 | INFO     | __main__:search_user_from_query:6 - query = Lagos-Calabar Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 22/108 [28:07<2:12:51, 92.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:55:34.656 | INFO     | __main__:search_user_from_query:6 - query = Lagos-Kano Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██▏       | 23/108 [29:51<2:16:15, 96.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:57:18.981 | INFO     | __main__:search_user_from_query:6 - query = Chad-Cameroon & Chad-Sudan Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 24/108 [31:36<2:18:14, 98.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:59:03.716 | INFO     | __main__:search_user_from_query:6 - query = Addis Ababa Light Rail\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = LightRailAddis\n",
      "parsing uid = ercaalrts\n",
      "parsing uid = LightRailAddis\n",
      "parsing uid = ercaalrts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 25/108 [31:52<1:42:26, 74.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 12:59:20.139 | INFO     | __main__:search_user_from_query:6 - query = Benguela Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 26/108 [33:36<1:53:24, 82.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:01:03.957 | INFO     | __main__:search_user_from_query:6 - query = Abuja-Kaduna Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 27/108 [35:20<2:00:36, 89.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:02:48.138 | INFO     | __main__:search_user_from_query:6 - query = Khartoum-Port Sudan Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 28/108 [37:04<2:04:58, 93.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:04:32.135 | INFO     | __main__:search_user_from_query:6 - query = Djibouti-Ethiopia Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 29/108 [38:48<2:07:30, 96.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:06:16.210 | INFO     | __main__:search_user_from_query:6 - query = Vientane-Boten Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 30/108 [40:32<2:08:40, 98.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:08:00.180 | INFO     | __main__:search_user_from_query:6 - query = Savannakhet-Lao Bao Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 31/108 [42:16<2:08:55, 100.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:09:44.100 | INFO     | __main__:search_user_from_query:6 - query = Bangkok-Nong Khai Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = JeffMcClung\n",
      "parsing uid = Air1Lauren\n",
      "parsing uid = ThisIsJLiv\n",
      "parsing uid = iamedee\n",
      "parsing uid = JCraig_Dallas\n",
      "parsing uid = bjacksondigital\n",
      "parsing uid = MarriedMornings\n",
      "parsing uid = air1radio\n",
      "parsing uid = JeffMcClung\n",
      "parsing uid = Air1Lauren\n",
      "parsing uid = ThisIsJLiv\n",
      "parsing uid = iamedee\n",
      "parsing uid = JCraig_Dallas\n",
      "parsing uid = bjacksondigital\n",
      "parsing uid = MarriedMornings\n",
      "parsing uid = air1radio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 32/108 [42:34<1:35:47, 75.63s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:10:01.786 | INFO     | __main__:search_user_from_query:6 - query = Bangkok-Chiang Mai Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 33/108 [44:18<1:45:12, 84.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:11:45.888 | INFO     | __main__:search_user_from_query:6 - query = Kuala Lumpur-Singapore High Speed Rail\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███▏      | 34/108 [46:02<1:51:14, 90.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:13:30.160 | INFO     | __main__:search_user_from_query:6 - query = Jakarta-Bandung Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = jakarta52325993\n",
      "parsing uid = info_DKI\n",
      "parsing uid = kelkebonwaru\n",
      "parsing uid = jakartagoid\n",
      "parsing uid = PemkotBandung\n",
      "parsing uid = jakarta52325993\n",
      "parsing uid = info_DKI\n",
      "parsing uid = kelkebonwaru\n",
      "parsing uid = jakartagoid\n",
      "parsing uid = PemkotBandung\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 35/108 [46:19<1:23:00, 68.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:13:47.109 | INFO     | __main__:search_user_from_query:6 - query = East Coast Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n",
      "parsing uid = eastcoastrail\n",
      "parsing uid = das_nirakar1963\n",
      "parsing uid = EastShramik\n",
      "parsing uid = Trainhelp1\n",
      "parsing uid = ECoastRailway\n",
      "parsing uid = rpfecor1\n",
      "parsing uid = jpmrail\n",
      "parsing uid = National_Rail\n",
      "parsing uid = eastcoastrail\n",
      "parsing uid = das_nirakar1963\n",
      "parsing uid = EastShramik\n",
      "parsing uid = Trainhelp1\n",
      "parsing uid = ECoastRailway\n",
      "parsing uid = rpfecor1\n",
      "parsing uid = jpmrail\n",
      "parsing uid = National_Rail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 36/108 [46:37<1:03:36, 53.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:14:04.606 | INFO     | __main__:search_user_from_query:6 - query = Gemas-Johor Bahru Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 37/108 [48:21<1:20:48, 68.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:15:48.568 | INFO     | __main__:search_user_from_query:6 - query = Dawei Port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 38/108 [50:05<1:32:06, 78.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:17:32.381 | INFO     | __main__:search_user_from_query:6 - query = Gujarat Rural Roads (MMGSY) Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 39/108 [51:49<1:39:26, 86.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:19:16.385 | INFO     | __main__:search_user_from_query:6 - query = Nurek Hydropower Rehabilitation Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 40/108 [53:33<1:43:58, 91.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:21:00.443 | INFO     | __main__:search_user_from_query:6 - query = Batumi Bypass Road Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 41/108 [55:17<1:46:36, 95.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:22:44.606 | INFO     | __main__:search_user_from_query:6 - query = Natural Gas Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = barbarosdemir\n",
      "parsing uid = PlattsGas\n",
      "parsing uid = tanapofficial\n",
      "parsing uid = NESupplyEnhance\n",
      "parsing uid = Cameron_LNG\n",
      "parsing uid = naturalgasENRG\n",
      "parsing uid = AtlSunProject\n",
      "parsing uid = PPONews\n",
      "parsing uid = powerimpossible\n",
      "parsing uid = barbarosdemir\n",
      "parsing uid = PlattsGas\n",
      "parsing uid = tanapofficial\n",
      "parsing uid = NESupplyEnhance\n",
      "parsing uid = Cameron_LNG\n",
      "parsing uid = naturalgasENRG\n",
      "parsing uid = AtlSunProject\n",
      "parsing uid = PPONews\n",
      "parsing uid = powerimpossible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 42/108 [55:35<1:19:29, 72.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:23:02.713 | INFO     | __main__:search_user_from_query:6 - query = Tarbela 5 Hydropower Extension Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███▉      | 43/108 [57:19<1:28:34, 81.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:24:46.634 | INFO     | __main__:search_user_from_query:6 - query = M4 Motorway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = TheM4Motorway\n",
      "parsing uid = M4motorway\n",
      "parsing uid = LlangyfelachPri\n",
      "parsing uid = MadejskiHotel\n",
      "parsing uid = westconnex\n",
      "parsing uid = TheM4Motorway\n",
      "parsing uid = M4motorway\n",
      "parsing uid = LlangyfelachPri\n",
      "parsing uid = MadejskiHotel\n",
      "parsing uid = westconnex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 44/108 [57:36<1:06:26, 62.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:25:03.481 | INFO     | __main__:search_user_from_query:6 - query = Dushanbe-Uzbekistan Border Road Improvement\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 45/108 [59:20<1:18:35, 74.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:26:47.647 | INFO     | __main__:search_user_from_query:6 - query = Nenskra Hydropower Plant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 46/108 [1:01:06<1:26:55, 84.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:28:33.389 | INFO     | __main__:search_user_from_query:6 - query = Amaravati Sustainable Capital City\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▎     | 47/108 [1:02:49<1:31:34, 90.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:30:17.348 | INFO     | __main__:search_user_from_query:6 - query = Madhya Pradesh Rural Connectivity Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 48/108 [1:04:34<1:34:20, 94.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:32:01.659 | INFO     | __main__:search_user_from_query:6 - query = Mumbai Metro Line 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 49/108 [1:06:18<1:35:38, 97.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:33:45.748 | INFO     | __main__:search_user_from_query:6 - query = Sahiwal 2x660MW Coal-fired Power Plant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▋     | 50/108 [1:08:02<1:36:01, 99.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:35:29.906 | INFO     | __main__:search_user_from_query:6 - query = UEP 100MW Wind Farm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 51/108 [1:09:46<1:35:39, 100.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:37:13.791 | INFO     | __main__:search_user_from_query:6 - query = Sachal 50MW Wind Farm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 52/108 [1:11:30<1:34:57, 101.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:38:57.953 | INFO     | __main__:search_user_from_query:6 - query = Peshawar-Karachi Motorway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 53/108 [1:13:14<1:33:51, 102.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:40:41.894 | INFO     | __main__:search_user_from_query:6 - query = Havelian Dry Port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 54/108 [1:14:58<1:32:32, 102.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:42:25.730 | INFO     | __main__:search_user_from_query:6 - query = Gwadar International Airport\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 55/108 [1:16:42<1:31:06, 103.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:44:09.590 | INFO     | __main__:search_user_from_query:6 - query = Myitsone Dam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 56/108 [1:18:26<1:29:36, 103.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:45:53.585 | INFO     | __main__:search_user_from_query:6 - query = Balloki Power Plant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 57/108 [1:20:10<1:28:01, 103.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:47:37.518 | INFO     | __main__:search_user_from_query:6 - query = Gadani Power Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▎    | 58/108 [1:21:54<1:26:23, 103.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:49:21.427 | INFO     | __main__:search_user_from_query:6 - query = Hakla–Dera Ismail Khan Motorway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▍    | 59/108 [1:23:38<1:24:45, 103.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:51:05.519 | INFO     | __main__:search_user_from_query:6 - query = Khunjerab Railway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 60/108 [1:25:22<1:23:03, 103.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:52:49.390 | INFO     | __main__:search_user_from_query:6 - query = M5 Motorway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n",
      "parsing uid = TrafficNewsM5\n",
      "parsing uid = HenburyGC\n",
      "parsing uid = M5TrafficUK\n",
      "parsing uid = westconnex\n",
      "parsing uid = BWTiverton\n",
      "parsing uid = TrafficNewsM5\n",
      "parsing uid = HenburyGC\n",
      "parsing uid = M5TrafficUK\n",
      "parsing uid = westconnex\n",
      "parsing uid = BWTiverton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▋    | 61/108 [1:25:38<1:00:51, 77.68s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:53:06.099 | INFO     | __main__:search_user_from_query:6 - query = M8 Motorway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n",
      "parsing uid = M8motorway\n",
      "parsing uid = M8_Scotland\n",
      "parsing uid = M8motorway\n",
      "parsing uid = M8_Scotland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 62/108 [1:25:55<45:29, 59.33s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:53:22.602 | INFO     | __main__:search_user_from_query:6 - query = Matiari–Lahore Transmission Line\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到头了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 63/108 [1:27:39<54:34, 72.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:55:06.711 | INFO     | __main__:search_user_from_query:6 - query = Orange Line Lahore Metro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 64/108 [1:29:23<1:00:14, 82.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:56:50.757 | INFO     | __main__:search_user_from_query:6 - query = Pak-China Technical and Vocational Institute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 65/108 [1:31:07<1:03:36, 88.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 13:58:34.950 | INFO     | __main__:search_user_from_query:6 - query = Pakistan Port Qasim Power Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 66/108 [1:32:51<1:05:20, 93.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2019-12-21 14:00:18.953 | INFO     | __main__:search_user_from_query:6 - query = Quaid-e-Azam Solar Park\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException\n"
     ]
    }
   ],
   "source": [
    "finish_query_list = []\n",
    "bad_query_list = []\n",
    "MAX_USER_SIZE = 20\n",
    "search_user_from_query(browser,query_list,finish_query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Doraleh Multi-Purpose Port',\n",
       " 'Forest City',\n",
       " 'Melaka Gateway',\n",
       " 'Gwadar Port',\n",
       " 'Belgrade-Montenegro Bar Port Motorway',\n",
       " 'Port City Colombo']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 采集用户主页推文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_user_list = []\n",
    "bad_query_list = []\n",
    "\n",
    "query_user_list = [u for u in user_table.find({\"query\":query})][:1]# query 不为空的user\n",
    "\n",
    "MAX_TWEET_SIZE = 50\n",
    "time_interval = \"2019年1月1日\" # 默认截止到今日\n",
    "# search_tweet_from_profile(browser,query_user_list,finish_user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
