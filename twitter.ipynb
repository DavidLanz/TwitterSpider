{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import pymongo\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from loguru import logger\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException,StaleElementReferenceException\n",
    "\n",
    "class Tweet:\n",
    "\n",
    "    def __init__(self, query, uid, ptime, pcontent, padditional, nb_reply, nb_retweet, nb_favorite):\n",
    "        self.query = query\n",
    "        self.uid = uid\n",
    "        self.ptime = ptime\n",
    "        self.pcontent = pcontent\n",
    "        self.padditional = padditional  # 转发推文，文章链接，图片，视频\n",
    "        self.nb_retweet = nb_retweet  # nbr of retweet\n",
    "        self.nb_favorite = nb_favorite  # nbr of favorite\n",
    "        self.nb_reply = nb_reply    # nbr of reply\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Tweet={}\\nQuery={}\".format(self.pcontent, self.query)\n",
    "\n",
    "\n",
    "class User:\n",
    "\n",
    "    def __init__(self, profile_url):\n",
    "        self.profile_url = profile_url\n",
    "        self.ID = profile_url.split('/')[-1]\n",
    "        self.name = ''\n",
    "        self.avatar = ''\n",
    "        self.query = ''# query相关的大V用户\n",
    "        self.intro = ''\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"User {}\".format(self.ID)\n",
    "\n",
    "\n",
    "def compare_time(time1,time2):\n",
    "    s_time = time.mktime(time.strptime(time1,'%Y年%m月%d日'))\n",
    "    e_time = time.mktime(time.strptime(time2,'%Y年%m月%d日'))\n",
    "    return int(s_time) - int(e_time)\n",
    "    \n",
    "def convert_time(x):\n",
    "    '''\n",
    "    for x in ['20分钟','1小时','1天', '10月10日','2018年10月1日']:\n",
    "        print(convert_time(x))\n",
    "    '''\n",
    "    now = datetime.datetime.now()\n",
    "    pattern = r'\\d{4}年\\d+月\\d+日'\n",
    "    if re.match(pattern, x):\n",
    "        return x\n",
    "    pattern = r'\\d+月\\d+日'\n",
    "    if re.match(pattern, x):\n",
    "        return \"{}年\".format(now.year)+x\n",
    "    return \"{}年{}月{}日\".format(now.year, now.month, now.day)\n",
    "\n",
    "def is_non_result(browser):\n",
    "    '''\n",
    "    判断结果是否为空\n",
    "    '''\n",
    "    result_div_xpath = \"//div[@id='react-root']\"\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "    try:\n",
    "        result_div = browser.find_element_by_xpath(result_div_xpath)\n",
    "        return '没有符合搜索条件的结果' in result_div.text\n",
    "    except NoSuchElementException as e:\n",
    "        return False\n",
    "\n",
    "def get_search_input_v1(browser):\n",
    "    # 定位搜索框\n",
    "    search_input_xpath = \"//input[@placeholder='搜索 Twitter']\"\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, search_input_xpath)))\n",
    "    search_input = browser.find_element_by_xpath(search_input_xpath)\n",
    "    return search_input\n",
    "\n",
    "# def get_search_input_v2(browser):\n",
    "#     # 请求主站\n",
    "#     browser.get('https://twitter.com/search-home')\n",
    "#     # 定位搜索框\n",
    "#     search_input_id = 'search-home-input'\n",
    "#     wait.until(EC.presence_of_element_located((By.ID, search_input_id)))\n",
    "#     search_input = browser.find_element_by_id(search_input_id)\n",
    "#     return search_input\n",
    "\n",
    "\n",
    "def extract_reply_retweet_favorite(element):\n",
    "    t = []\n",
    "    for x in element.find_elements_by_xpath('./div')[:3]:\n",
    "        if x.text.strip() == '':\n",
    "            t.append(0)\n",
    "        else:\n",
    "            t.append(int(x.text.strip()))\n",
    "    return tuple(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query -> 推文爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweet_result_div(result_div,query):\n",
    "    count = 0\n",
    "    for div in result_div:\n",
    "        user, tweet = div.find_elements_by_xpath('./div')\n",
    "        profile_url = user.find_element_by_tag_name(\n",
    "            'a').get_attribute('href').strip()\n",
    "        uid = profile_url.split('/')[-1]\n",
    "        a, *b_c, d = tweet.find_elements_by_xpath('./div')  # 按照div分为>=3层\n",
    "        ptime = a.find_elements_by_tag_name('a')[-1].text\n",
    "        ptime = convert_time(ptime)\n",
    "        nb_reply, nb_retweet, nb_favorite = 0,0,0\n",
    "        try:\n",
    "            nb_reply, nb_retweet, nb_favorite = extract_reply_retweet_favorite(\n",
    "                d)\n",
    "        except:\n",
    "            nb_reply, nb_retweet, nb_favorite = 0, 0, 0\n",
    "        pcontent = b_c[0].text\n",
    "        padditional = []\n",
    "        if len(b_c) > 1:\n",
    "            for x in b_c[1:]:\n",
    "                try:\n",
    "                    a = x.find_element_by_tag_name('a').get_attribute('href')\n",
    "                    padditional.append(a)\n",
    "                except NoSuchElementException as e:\n",
    "                    padditional.append(x.text.strip())\n",
    "        user = User(profile_url)\n",
    "        tweet = Tweet(query, uid, ptime, pcontent, padditional,\n",
    "                      nb_reply, nb_retweet, nb_favorite)\n",
    "        # save to databse\n",
    "        user_dict = user.__dict__\n",
    "        user_dict['_id']=user_dict['ID']\n",
    "        if user_table.update_one({'_id': user_dict['_id']}, {'$set': user_dict}, upsert=True) and tweet_table.insert_one(tweet.__dict__):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def crawl_tweet(browser, query):\n",
    "    count = 0\n",
    "    result_div_xpath = '//div[@data-testid=\"tweet\"]'\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "    result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "    last_div = result_div[-1]\n",
    "    # 解析结果\n",
    "    count += parse_tweet_result_div(result_div,query)\n",
    "    while count < MAX_TWEET_SIZE:\n",
    "        logger.info(\"{}/{}\".format(count,MAX_TWEET_SIZE))\n",
    "        result_div_xpath = '//div[@data-testid=\"tweet\"]'\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "        result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "        last_div = result_div[-1]\n",
    "        try:\n",
    "            count += parse_tweet_result_div(result_div,query)\n",
    "        except StaleElementReferenceException as e:\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        \n",
    "        # 翻页\n",
    "        while True:\n",
    "            browser.execute_script(\n",
    "                'window.scrollTo(0,document.body.scrollHeight)')\n",
    "            wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, result_div_xpath)))\n",
    "            result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "            if result_div[-1] != last_div:\n",
    "                last_div = result_div[-1]\n",
    "                break\n",
    "\n",
    "def search_tweet_from_query(browser,query_list):\n",
    "    '''\n",
    "    更加query采集推文\n",
    "    '''\n",
    "    for query in tqdm(query_list):\n",
    "        logger.info('query = {}'.format(query))\n",
    "        browser.get('https://twitter.com/explore')\n",
    "\n",
    "        # 定位搜索框\n",
    "        if browser.current_url == 'https://twitter.com/explore':\n",
    "            search_input = get_search_input_v1(browser)\n",
    "        else:\n",
    "            print('error')\n",
    "            return\n",
    "        # 搜索query\n",
    "        search_input.clear()\n",
    "        search_input.send_keys(query)\n",
    "        search_input.send_keys(Keys.ENTER)\n",
    "\n",
    "        # 获取结果\n",
    "        if is_non_result(browser):\n",
    "            bad_query_list.append(query)\n",
    "            continue\n",
    "        time.sleep(1)\n",
    "        crawl_tweet(browser, query)\n",
    "    print(bad_query_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query -> 爬取相关用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_user_from_query(browser,query_list):\n",
    "    '''\n",
    "    根据query采集maxsize用户列表\n",
    "    '''\n",
    "    for query in tqdm(query_list):\n",
    "        logger.info('query = {}'.format(query))\n",
    "        browser.get('https://twitter.com/explore')\n",
    "\n",
    "        # 定位搜索框\n",
    "        if browser.current_url == 'https://twitter.com/explore':\n",
    "            search_input = get_search_input_v1(browser)\n",
    "        else:\n",
    "            print('error')\n",
    "            return\n",
    "        # 搜索query\n",
    "        search_input.clear()\n",
    "        search_input.send_keys(query)\n",
    "        search_input.send_keys(Keys.ENTER)\n",
    "\n",
    "        # 获取结果\n",
    "        if is_non_result(browser):\n",
    "            bad_query_list.append(query)\n",
    "            continue\n",
    "        time.sleep(1)\n",
    "        # 请求用户结果页面\n",
    "        browser.get(browser.current_url + '&f=user')\n",
    "        crawl_user(browser,query)\n",
    "    \n",
    "    print(bad_query_list)\n",
    "\n",
    "def parse_user_result_div(result_div,query):\n",
    "    count = 0\n",
    "    for t in result_div:\n",
    "        left, right = t.find_elements_by_xpath('./div/div')\n",
    "        profile_url = left.find_element_by_tag_name(\n",
    "            'a').get_attribute('href').strip()\n",
    "        uid = profile_url.split('/')[-1]\n",
    "        print('parsing uid = {}'.format(uid))\n",
    "        avatar = left.find_element_by_tag_name('img').get_attribute('src')\n",
    "        a, *b = right.find_elements_by_xpath('./div')  # 按照div分为>=3层\n",
    "        uname= a.text.split('\\n')[0]\n",
    "        intro = ''\n",
    "        if len(b)!=0:\n",
    "            intro = b[-1].text.strip()\n",
    "        \n",
    "        user = User(profile_url)\n",
    "        user.ID = uid\n",
    "        user.avatar = avatar\n",
    "        user.name = uname\n",
    "        user.intro = intro\n",
    "        user.query = query\n",
    "        \n",
    "        user_dict = user.__dict__\n",
    "        user_dict['_id']=user_dict['ID']\n",
    "\n",
    "        # save to databse\n",
    "        if user_table.update_one({'_id': user_dict['_id']}, {'$set': user_dict}, upsert=True):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def crawl_user(browser, query):\n",
    "    count = 0\n",
    "    result_div_xpath = '//div[@data-testid=\"UserCell\"]'\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "    result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "    last_div = result_div[-1]\n",
    "    count += parse_user_result_div(result_div,query)\n",
    "    \n",
    "    while count < MAX_USER_SIZE:\n",
    "        logger.info(\"{}/{}\".format(count,MAX_USER_SIZE))\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "        result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "        last_div = result_div[-1]\n",
    "        \n",
    "        try:\n",
    "            count += parse_user_result_div(result_div,query)\n",
    "        except StaleElementReferenceException as e:\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        \n",
    "        # 翻页\n",
    "        while True:\n",
    "            browser.execute_script(\n",
    "                'window.scrollTo(0,document.body.scrollHeight)')\n",
    "            wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, result_div_xpath)))\n",
    "            result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "            if result_div[-1] != last_div:\n",
    "                last_div = result_div[-1]\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 爬取特定用户的推文（时间间隔内）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweet_from_profile(browser,query_user_list):\n",
    "    for user in tqdm(query_user_list):\n",
    "        logger.info('user = {}'.format(u['_id']))\n",
    "        user_profile = 'https://twitter.com/'+ u['_id']\n",
    "        browser.get(user_profile)\n",
    "\n",
    "        # 获取结果\n",
    "        if is_non_result(browser):\n",
    "            bad_query_list.append(query)\n",
    "            continue\n",
    "        time.sleep(1)\n",
    "        crawl_tweet2(browser,query)\n",
    "    print(bad_query_list)\n",
    "\n",
    "def crawl_tweet2(browser, query):\n",
    "    count = 0\n",
    "    result_div_xpath = '//div[@data-testid=\"tweet\"]'\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "    result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "    last_div = result_div[-1]\n",
    "    # 解析结果\n",
    "    count += parse_tweet_from_profile(result_div,query)\n",
    "    while count < MAX_TWEET_SIZE:\n",
    "        logger.info(\"{}/{}\".format(count,MAX_TWEET_SIZE))\n",
    "        result_div_xpath = '//div[@data-testid=\"tweet\"]'\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, result_div_xpath)))\n",
    "        result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "        last_div = result_div[-1]\n",
    "        try:\n",
    "            count += parse_tweet_from_profile(result_div,query)\n",
    "        except StaleElementReferenceException as e:\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        \n",
    "        # 翻页\n",
    "        while True:\n",
    "            browser.execute_script(\n",
    "                'window.scrollTo(0,document.body.scrollHeight)')\n",
    "            wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, result_div_xpath)))\n",
    "            result_div = browser.find_elements_by_xpath(result_div_xpath)\n",
    "            if result_div[-1] != last_div:\n",
    "                last_div = result_div[-1]\n",
    "                break\n",
    "def parse_tweet_from_profile(result_div,query):\n",
    "    count = 0\n",
    "    top = 0 # 置顶是个数目\n",
    "    # 如果存在置顶推文则不考虑时间\n",
    "    try:\n",
    "        t = browser.find_elements_by_xpath('//div[@class=\"css-1dbjc4n r-1habvwh r-1iusvr4 r-16y2uox r-5f2r5o\"]')\n",
    "        top = len(t)\n",
    "    except NoSuchElementException as e:\n",
    "        pass\n",
    "    \n",
    "    for div in result_div:\n",
    "        user, tweet = div.find_elements_by_xpath('./div')\n",
    "        profile_url = user.find_element_by_tag_name(\n",
    "            'a').get_attribute('href').strip()\n",
    "        uid = profile_url.split('/')[-1]\n",
    "        a, *b_c, d = tweet.find_elements_by_xpath('./div')  # 按照div分为>=3层\n",
    "        ptime = a.find_elements_by_tag_name('a')[-1].text\n",
    "        ptime = convert_time(ptime)\n",
    "        \n",
    "        # 无置顶推文则按照时间过滤\n",
    "        top -= 1\n",
    "        if top<0 and compare_time(ptime,time_interval) < 0:\n",
    "            print('触发时间截止')\n",
    "            return MAX_TWEET_SIZE # 使其直接达到数目规模，从而停止外层循环\n",
    "        \n",
    "        nb_reply, nb_retweet, nb_favorite = 0,0,0\n",
    "        try:\n",
    "            nb_reply, nb_retweet, nb_favorite = extract_reply_retweet_favorite(\n",
    "                d)\n",
    "        except:\n",
    "            nb_reply, nb_retweet, nb_favorite = 0, 0, 0\n",
    "        pcontent = b_c[0].text\n",
    "        padditional = []\n",
    "        if len(b_c) > 1:\n",
    "            for x in b_c[1:]:\n",
    "                try:\n",
    "                    a = x.find_element_by_tag_name('a').get_attribute('href')\n",
    "                    padditional.append(a)\n",
    "                except NoSuchElementException as e:\n",
    "                    padditional.append(x.text.strip())\n",
    "        tweet = Tweet(query, uid, ptime, pcontent, padditional,\n",
    "                      nb_reply, nb_retweet, nb_favorite)\n",
    "\n",
    "        if tweet_table.insert_one(tweet.__dict__):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 启动浏览器并登陆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://10.108.17.25:27017/\")\n",
    "twitter_db = client[\"twitter\"]\n",
    "user_table = twitter_db['user']\n",
    "tweet_table = twitter_db['tweet']\n",
    "\n",
    "# 打开浏览器\n",
    "browser = webdriver.Chrome()\n",
    "wait = WebDriverWait(browser, 100)\n",
    "\n",
    "# 人工登录\n",
    "browser.get('https://twitter.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.refresh()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 采集推文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_query_list = []\n",
    "MAX_TWEET_SIZE = 50\n",
    "query_list = ['the belt and road']\n",
    "# search_tweet_from_query(browser,query_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 采集用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_query_list = []\n",
    "MAX_USER_SIZE = 10\n",
    "query_list = ['the belt and road']\n",
    "# search_user_from_query(browser,query_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 采集用户主页推文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_query_list = []\n",
    "query = \"the belt and road\"\n",
    "query_user_list = [u for u in user_table.find({\"query\":query})][:1]\n",
    "\n",
    "MAX_TWEET_SIZE = 50\n",
    "time_interval = \"2019年1月1日\" # 默认截止到今日\n",
    "# search_tweet_from_profile(browser,query_user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
